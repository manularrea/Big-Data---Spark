{"cells":[{"cell_type":"markdown","metadata":{"id":"nr0IzsZSEXDW"},"source":["# Pair RDD - Actions"]},{"cell_type":"markdown","source":["## Stup Spark environment"],"metadata":{"id":"TzbSMLfdEcj6"}},{"cell_type":"code","source":["from pathlib import Path\n","\n","installation_folder = Path(\"/content/spark-3.5.0-bin-hadoop3\")\n","\n","if not installation_folder.exists():\n","\n","  # Install Java locally\n","  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","  # Download & decompress Spark\n","  !wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz -O spark-3.5.0-bin-hadoop3.tgz\n","  !tar xf spark-3.5.0-bin-hadoop3.tgz\n","\n","  # Install finspark\n","  !pip install -q findspark\n","\n","  # Setup required environment variables\n","  import os\n","  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","  os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n","\n","  print(\"Spark setup finished!\")\n","\n","else:\n","  print(\"Skipping Spark setup\")"],"metadata":{"id":"Zkhh5R3bEcWQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702582830950,"user_tz":-60,"elapsed":34213,"user":{"displayName":"Miguel Ángel Corella","userId":"12810348842830866806"}},"outputId":"2f522f0c-2144-4744-c8ef-3561a7305073"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Spark setup finished!\n"]}]},{"cell_type":"markdown","metadata":{"id":"ixyYNsqpEXDY"},"source":["## Prepare the Spark context"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"LSzn3t9rEXDY","executionInfo":{"status":"ok","timestamp":1702582842056,"user_tz":-60,"elapsed":11114,"user":{"displayName":"Miguel Ángel Corella","userId":"12810348842830866806"}}},"outputs":[],"source":["# Import findpsark\n","import findspark\n","\n","# Configure the environment\n","findspark.init()\n","\n","# Import the Spark components required for the context creation\n","from pyspark import SparkConf, SparkContext\n","\n","# Configure and create the context\n","conf = SparkConf()\n","conf = conf.setAppName('mds-session')\n","conf = conf.setMaster('local[*]')\n","sc = SparkContext.getOrCreate(conf=conf)"]},{"cell_type":"markdown","metadata":{"id":"GYxs-2EhEXDZ"},"source":["## countByKey"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"yQ5AWNdmEXDa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702582846535,"user_tz":-60,"elapsed":4506,"user":{"displayName":"Miguel Ángel Corella","userId":"12810348842830866806"}},"outputId":"7f9a6fd5-cce9-42b4-8677-feb30c875b73"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(int, {'2014': 2, '2015': 1, '2016': 2, '2017': 1})"]},"metadata":{},"execution_count":3}],"source":["rdd1 = sc.parallelize(['2014-12-31', '2015-01-25', '2016-05-17', '2016-11-08', '2017-01-05', '2014-08-06'])\n","\n","def parseDate(date):\n","    year, month, day = date.split('-')\n","    return (year, month + '-' + day)\n","\n","rdd2 = rdd1.map(parseDate)\n","rdd2.countByKey()"]},{"cell_type":"markdown","metadata":{"id":"xvaPpEZlEXDa"},"source":["## Close the Spark context"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIBfb9R-EXDa"},"outputs":[],"source":["sc.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[{"file_id":"1KVX1J23icYIJ9Ct5IhhU94SrfuZov8yc","timestamp":1702200705793}]}},"nbformat":4,"nbformat_minor":0}